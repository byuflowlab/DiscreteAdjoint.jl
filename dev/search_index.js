var documenterSearchIndex = {"docs":
[{"location":"guide/#guide","page":"Example Usage","title":"Example Usage","text":"","category":"section"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"The main function which this package exports is the discrete_adjoint function.","category":"page"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"discrete_adjoint","category":"page"},{"location":"guide/#DiscreteAdjoint.discrete_adjoint","page":"Example Usage","title":"DiscreteAdjoint.discrete_adjoint","text":"discrete_adjoint(sol, dg, t; autojacvec=ForwardDiffVJP(), kwargs...)\n\nComputes the discrete adjoint for the solution object sol.\n\nArguments:\n\nsol: Solution object from DifferentialEquations.  Note that the provided solution   must save every time step.\ndg: A function of the form dg(dgval, x, p, t, i) which returns the partial   derivatives of the objective/loss function with respect to the state variables   at the ith time step in t\nt: Time steps at which the objective/loss function is evaluated.  When solving the   original differential equation, a tstop must be set for each time in t\n\nKeyword Arguments\n\nautojacvec: Method by which to compute the vector-transposed jacobian product.   Possible choices are:\nForwardDiffVJP(): Forward-mode automatic differentiation using ForwardDiff,\nReverseDiffVJP(compile=true): Reverse-mode automatic differentiation using ReverseDiff\nZygoteVJP: Reverse-mode automatic differentiation using Zygote\nIn general, reverse-mode automatic differentiation should be faster than forward-mode   automatic differentiation, especially when large numbers of parameters are considered. \n\n\n\n\n\n","category":"function"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"The following are a few examples of how to use this function.","category":"page"},{"location":"guide/#Lotka-Volterra-Model","page":"Example Usage","title":"Lotka-Volterra Model","text":"","category":"section"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"Our first example problem is the non-stiff Lotka-Volterra model","category":"page"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"beginaligned\nfracdxdt = p_1 x - p_2 x y \nfracdydt = -p_3 y + x y\nendaligned","category":"page"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"with initial condition u_0 = 10 10 and p = 15 10 30.","category":"page"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"We use an L^2 objective/loss function sampled at 100 evenly space points.","category":"page"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"using OrdinaryDiffEq, DiscreteAdjoint\n\n# lotka volterra model\nf = (du, u, p, t) -> begin\n    du[1] = p[1]*u[1] - p[2]*u[1]*u[2]\n    du[2] = -p[3]*u[2] + p[4]*u[1]*u[2]\nend\nu0 = [1.0, 1.0]; p = [1.5,1.0,3.0,1.0]; tspan = (0.0, 10.0); \nprob = ODEProblem(f, u0, tspan, p)\n\n# times at which to evaluate the solution\nt = range(tspan[1], tspan[2], length=100)\n\n# solve the ODEProblem\nsol = solve(prob, Tsit5(), u0=u0, p=p, abstol=1e-10, reltol=1e-10, tstops=t)\n\n# objective/loss function (not used, but shown for clarity)\nfunction sum_of_solution(x)\n    _prob = remake(prob, u0=x[1:2], p=x[3:end])\n    sum(solve(_prob, Tsit5(), abstol=1e-10, reltol=1e-10, saveat=t))\nend\n\n# gradient of the objective function w.r.t the state variables from a specific time step\ndg(out,u,p,t,i) = out .= 1\n\n# adjoint solution using ReverseDiffVJP()\ndp_rd, du0_rd = discrete_adjoint(sol, dg, t; autojacvec=ReverseDiffVJP())","category":"page"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"Note that this problem was adopted from http://dx.doi.org/10.1109/HPEC49654.2021.9622796","category":"page"},{"location":"guide/#Brusselator-Model","page":"Example Usage","title":"Brusselator Model","text":"","category":"section"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"Our second example problem is the two dimensional (N times N) Brusselator stiff reaction-diffusion PDE:","category":"page"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"beginaligned\nfracpartial upartial t = p_2 + u^2 v - (p_1 + 1) u + p_3 ( fracpartial^2 upartial x^2  + fracpartial^2 upartial y^2) + f(x y t) \nfracpartial vpartial t = p_1 u - u^2 v + p_4 ( fracpartial^2 upartial x^2 + fracpartial^2 upartial y^2)\nendaligned","category":"page"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"where","category":"page"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"f(xyt) = begincases\n5  textif  (x-03)^2 + (y-06)^2 leq 01^2 text and  t geq 11 \n0  textelse \nendcases","category":"page"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"with no-flux boundary conditions and u(0 x y) = 22(y(1 - y))^32 with v(0 x y) = 27(x(1 - x))^32. This PDE is discretized to a set of N times N times 2 ODEs using the finite difference method. The parameters are spatially-dependent, p_i = p_i(x y), making each discretized p_i a N times N set of values at each discretization point, giving a total of 4 N^2 parameters. The initial parameter values are the uniform p_i(x y) = 34 10 100 100","category":"page"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"Once again, we use an L^2 objective/loss function sampled at 100 evenly space points.","category":"page"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"using OrdinaryDiffEq, DiscreteAdjoint\n\n# brusselator model\n\nN = 3\n\nxyd_brusselator = range(0,stop=1,length=N)\n\ndx = step(xyd_brusselator)\n\nbrusselator_f(x, y, t) = (((x-0.3)^2 + (y-0.6)^2) <= 0.1^2) * (t >= 1.1) * 5.\n\nlimit(a, N) = a == N+1 ? 1 : a == 0 ? N : a\n\nfunction brusselator_2d_loop(du, u, p, t)\n    lu = LinearIndices((1:N, 1:N, 1:2))\n    lp = LinearIndices((1:N, 1:N, 1:4))\n    @inbounds for I in CartesianIndices((N, N))\n        i, j = Tuple(I)\n        x, y = xyd_brusselator[I[1]], xyd_brusselator[I[2]]\n        ip1, im1, jp1, jm1 = limit(i+1, N), limit(i-1, N), limit(j+1, N), limit(j-1, N)\n        du[lu[i,j,1]] = p[lp[i,j,2]] + u[lu[i,j,1]]^2*u[lu[i,j,2]] - (p[lp[i,j,1]] + 1)*u[lu[i,j,1]] + \n            p[lp[i,j,3]]/dx^2*(u[lu[im1,j,1]] + u[lu[ip1,j,1]] + u[lu[i,jp1,1]] + u[lu[i,jm1,1]] - 4u[lu[i,j,1]]) +\n            brusselator_f(x, y, t)\n        du[lu[i,j,2]] = p[lp[i,j,1]]*u[lu[i,j,1]] - u[lu[i,j,1]]^2*u[lu[i,j,2]] + \n            p[lp[i,j,4]]/dx^2*(u[lu[im1,j,2]] + u[lu[ip1,j,2]] + u[lu[i,jp1,2]] + u[lu[i,jm1,2]] - 4u[lu[i,j,2]])\n    end\nend\n\npt = (3.4, 1., 10., 10.)\n\nfunction init_brusselator_2d(xyd, pt)\n    N = length(xyd)\n    u0 = zeros(N*N*2)\n    p = zeros(N*N*4)\n    ru0 = reshape(u0, N, N, 2)\n    rp = reshape(p, N, N, 4)\n    for I in CartesianIndices((N, N))\n        x = xyd[I[1]]\n        y = xyd[I[2]]\n        ru0[I,1] = 22*(y*(1-y))^(3/2)\n        ru0[I,2] = 27*(x*(1-x))^(3/2)\n        rp[I,1] = pt[1]\n        rp[I,2] = pt[2]\n        rp[I,3] = pt[3]\n        rp[I,4] = pt[4]\n    end\n    return u0, p\nend\n\nu0, p = init_brusselator_2d(xyd_brusselator, pt)\n\ntspan = (0.,10.0)\n\nprob_ode_brusselator_2d = ODEProblem(brusselator_2d_loop,u0,tspan,p)\n\n# times at which to evaluate the solution\nt = range(tspan[1], tspan[2], length=100)\n\n# solve the ODEProblem\nsol = solve(prob_ode_brusselator_2d, Tsit5(), abstol=1e-6, reltol=1e-6, tstops=t)\n\n# objective/loss function (not used, but shown for clarity)\nfunction sum_of_solution(x)\n    _prob = remake(prob, u0=x[1:length(u0)], p=x[length(u0)+1:end])\n    sol = solve(prob_ode_brusselator_2d, Tsit5(), abstol=1e-6, reltol=1e-6, saveat=t)\nend\n\n# gradient of the objective function w.r.t the state variables from a specific time step\ndg(out,u,p,t,i) = out .= 1\n\n# adjoint solution using ReverseDiffVJP()\ndp_rd, du0_rd = discrete_adjoint(sol, dg, t; autojacvec=ReverseDiffVJP())","category":"page"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"Note that this example was adopted from http://dx.doi.org/10.1109/HPEC49654.2021.9622796","category":"page"},{"location":"guide/#Robertson-Model","page":"Example Usage","title":"Robertson Model","text":"","category":"section"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"Our third example is the Robertson equation in its implicit form:","category":"page"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"beginaligned\nfracdy_1dt = -p_1 y_1 + p_2 y_2 y_3 \nfracdy_2dt = p_1  y_1 - p_2 y_2 y_3 - p_3 y_2^2 \n0 =  y_1 + y_2 + y_3 - p_4 \nendaligned","category":"page"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"with initial condition u_0 = [1.0, 0.0, 0.0] and parameters p = [0.04, 1e4, 3e7, 1.0].","category":"page"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"Once again, we use an L^2 objective/loss function sampled at 100 evenly space points.","category":"page"},{"location":"guide/","page":"Example Usage","title":"Example Usage","text":"\nusing OrdinaryDiffEq, DiscreteAdjoint\n\n# robertson model\nf = (out,du,u,p,t) -> begin\n    out[1] = - p[1]*u[1]               + p[2]*u[2]*u[3] - du[1]\n    out[2] = + p[1]*u[1] - p[3]*u[2]^2 - p[2]*u[2]*u[3] - du[2]\n    out[3] = u[1] + u[2] + u[3] - p[4]\nend\np0 = [0.04,1e4,3e7,1.0]; tspan=(1e-6,1e5); u0 = [1.0,0.0,0.0]; du0 = [-0.04,0.04,0.0];\nprob = DAEProblem(f, du0, u0, tspan, p0, differential_vars = [true,true,false])\n\n# times at which to evaluate the solution\nt = range(tspan[1], tspan[2], length=100)\n\n# solve the DAEProblem\nsol = solve(prob, DImplicitEuler(), u0=u0, p=p0, abstol=1e-6, reltol=1e-6, saveat=t, initializealg=NoInit())\n\n# objective/loss function (not used, but shown for clarity)\nfunction sum_of_solution(x)\n    _prob = remake(prob, u0=x[1:4], p=x[5:end])\n    sum(solve(_prob, DImplicitEuler(), abstol=1e-6, reltol=1e-6, saveat=t, initializealg=NoInit()))\nend\n\n# gradient of the objective function w.r.t the state variables from a specific time step\ndg(out,u,p,t,i) = out .= 1\n\n# adjoint solution using ReverseDiffVJP()\ndp_rd, du0_rd = discrete_adjoint(sol, dg, t; autojacvec=ReverseDiffVJP())\n","category":"page"},{"location":"theory/#theory","page":"Theory","title":"Discrete Adjoint Derivation","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"We can view the time integration of any ordinary differential equation as a collection of residual expressions representing an initialization step and a sequence of discrete integration steps from t_0 to t_n.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"$","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"R(x, p, t) = \\begin{bmatrix} r0(x0, p, t0) \\\nr1(x{0 \\cdots 1}, p, t{0 \\cdots 1}) \\\nr2(x{0 \\cdots 2}, p, t{0 \\cdots 2}) \\\nr3(x{0 \\cdots 3}, p, t{0 \\cdots 3}) \\\n\\vdots \\\nrn(x{0 \\cdots n}, p, t_{0 \\cdots n}) \\\n\\end{bmatrix} = 0 $","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We are interested in some function of interest (FOI) G(xp) which may be calculated from the values of the state variables throughout the simulation as well as the parameters.  ","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"We would like to obtain the total derivative of the FOI with respect to the parameters","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"$","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"\\frac{d G}{d p} = \\frac{\\partial G}{\\partial p} + \\frac{\\partial G}{\\partial x} \\frac{d x}{d p} $","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"An expression for the total derivative fracd xd p may be found by taking the total derivative of the residual expression R(x p t)","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"$","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"\\frac{d R}{d p} = \\frac{\\partial R}{\\partial p} + \\frac{\\partial R}{\\partial x} \\frac{d x}{d p} = 0 $","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"$","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"\\frac{d x}{d p} = -\\left(\\frac{\\partial R}{\\partial x}\\right)^{-1} \\frac{\\partial R}{\\partial p} $","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"Substituting this expression in for fracd xd p allows us to compute fracd Gd p using only partial derivatives.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"$","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"\\frac{d G}{d p} = \\frac{\\partial G}{\\partial p} - \\frac{\\partial G}{\\partial x} \\left(\\frac{\\partial R}{\\partial x}\\right)^{-1} \\frac{\\partial R}{\\partial p} $","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"For the adjoint method we first solve","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"$","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"\\lambda^* = \\frac{\\partial G}{\\partial x} \\left(\\frac{\\partial R}{\\partial x}\\right)^{-1} $","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"and then compute the total derivative as","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"$","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"\\frac{d G}{d p} = \\frac{\\partial G}{\\partial p} - \\lambda^* \\frac{\\partial R}{\\partial p} $","category":"page"},{"location":"theory/#Implementation","page":"Theory","title":"Implementation","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"In general, fracpartial Rpartial x is large lower-triangular square matrix, fracpartial Rpartial p is large tall matrix, and fracpartial Rpartial p is a large wide matrix. For performance and memory reasons, fracpartial Rpartial x, fracpartial Rpartial p, and fracpartial Rpartial p shouldn't be explictly constructed.  Instead, the sparsity structure of fracpartial Rpartial x should be exploited so that only a small portion of fracpartial Rpartial x, fracpartial Rpartial p, and fracpartial Rpartial p needs to be held in memory at the same time.  On the other hand, the partial derivative matrix fracpartial Gpartial p is relatively small and may therefore be constructed explicitly without introducing any significant performance penalties.  This section explains the procedure for calculating the total derivative without explicitly constructing fracpartial Rpartial x, fracpartial Rpartial p, or fracpartial Rpartial p.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The adjoint vector lambda may be formed by solving the following equation $ \\begin{bmatrix} {\\frac{\\partial r0}{\\partial x0}}^* & \\frac{\\partial r1}{\\partial x0}^* & \\cdots & \\frac{\\partial r{n-1}}{\\partial x0}^* & \\frac{\\partial r{n}}{\\partial x0}^* \\\n & \\frac{\\partial r1}{\\partial x1}^* & \\cdots & \\frac{\\partial r{n-1}}{\\partial x1}^* & \\frac{\\partial rn}{\\partial x1}^* \\\n &  & \\ddots & \\vdots & \\vdots \\\n  & & &  \\frac{\\partial r{n-1}}{\\partial x{n-1}}^* & \\frac{\\partial rn}{\\partial x{n-1}}^\\\n & & & & \\frac{\\partial rn}{\\partial xn}^ \\end{bmatrix} \\begin{bmatrix}   \\lambda0 \\\n  \\lambda1 \\\n  \\vdots & \\\n  \\lambda{n-1} \\\n  \\lambdan \\end{bmatrix}  =  \\begin{bmatrix}   \\frac{\\partial G}{\\partial x0}^* \\\n  \\frac{\\partial G}{\\partial x1}^* \\\n  \\vdots \\\n  \\frac{\\partial G}{\\partial x{n-1}}^* \\\n  \\frac{\\partial G}{\\partial xn}^* \\end{bmatrix} $ This matrix problem may be solved iteratively starting from the final time step using the following expression. $ \\frac{\\partial ri}{\\partial xi}^* \\lambdai = \\frac{\\partial G}{\\partial xi}^* - \\sum{k=i+1}^n \\left( \\lambdak^* \\frac{\\partial rk}{\\partial xi} \\right)^* $ In practice, state variables from only a limited number of  previous time steps are used to compute state variables for the current time step, so only a few terms from the summation are actually needed to compute the adjoint vector.  ","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The elements of the adjoint vector may be mutliplied by the elements of fracpartial Rpartial p and summed with fracpartial Gpartial p as they are computed to calculate the total derivative fracd Gd p $ \\frac{d G}{d p} = \\frac{\\partial G}{\\partial p} - \\sum{i=0}^n \\lambdai^* \\frac{\\partial r_i}{\\partial p} $","category":"page"},{"location":"theory/#Computing-Partial-Derivatives","page":"Theory","title":"Computing Partial Derivatives","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"The vector-transpose Jacobian products lambda_k^* fracpartial r_kpartial x_i and lambda_i^* fracpartial r_ipartial p may be computed efficiently without forming the Jacobian using reverse-mode automatic differentiation.  The remaining partial derivatives fracpartial r_ipartial x_i, fracpartial Gpartial x_i and fracpartial Gpartial p may be provided analytically or computed using numerical or automatic differentiation.","category":"page"},{"location":"theory/#Explicit-Midpoint-Method-with-an-Explicit-ODE","page":"Theory","title":"Explicit Midpoint Method with an Explicit ODE","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"The residual function for each time step takes the following form $ r(xi, x{i+1}, p, ti, t{i+1}) = x{i+1} - xi - (t{i+1} - t{i}) f \\left(xi + \\frac{ti + t{i+1}}{2} f(xi, p, ti), p, ti + \\frac{t{i+1} - t{i}}{2} \\right) $ where f is our underlying ordinary differential equation.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The jacobians fracpartial r_ipartial x_i, fracpartial r_ipartial x_i-1, and fracpartial r_ipartial p may then be defined analytically as","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"$","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"\\frac{\\partial ri}{\\partial x{i+1}} = I $ ","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"$","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"\\frac{\\partial ri}{\\partial x{i}} = -I - (t{i+1} - t{i}) \\left( \\frac{\\partial f}{\\partial x{i}} \\left(xi + \\frac{ti + t{i+1}}{2} f(xi, p, ti), p, ti + \\frac{t{i+1} - t{i}}{2} \\right) \\left(1 + \\frac{t{i+1} - t{i}}{2}\\frac{\\partial f}{\\partial x{i}}( xi, pi, t)  \\right) \\right) $ ","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"$","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"\\frac{\\partial ri}{\\partial p} = - (t{i+1} - t{i}) \\left(\\frac{\\partial f}{\\partial p} \\left(xi + \\frac{ti + t{i+1}}{2} f(xi, p, ti), p, ti + \\frac{t{i+1} - t{i}}{2} \\right) +    \\frac{\\partial f}{\\partial x} \\left(xi + \\frac{ti + t{i+1}}{2} f(xi, p, ti), p, ti + \\frac{t{i+1} - t{i}}{2} \\right) \\left(\\frac{ti + t{i+1}}{2} \\frac{\\partial f}{\\partial p}(xi, p, t_i)\\right) \\right) $","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The jacobians fracpartial fpartial x and fracpartial fpartial p may be provided analytically or computed using numerical or automatic differentiation.  Note that since this method is explicit, identity matrices will occupy the diagonal of fracpartial Rpartial x.  The adjoint vector may therefore be found without performing a linear solve to calculate each portion of the adjoint vector","category":"page"},{"location":"theory/#Implicit-Midpoint-Method-with-an-Explicit-ODE","page":"Theory","title":"Implicit Midpoint Method with an Explicit ODE","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"The residual function for each time step takes the following form $ r(xi, x{i+1}, p, ti, t{i+1}) = x{i+1} - xi - (t{i+1} - t{i}) f \\left( \\frac{xi + x{i+1}}{2}, p, ti + \\frac{t{i+1} - t_{i}}{2} \\right) $ where f is our underlying ordinary differential equation.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The jacobians fracpartial r_ipartial x_i, fracpartial r_ipartial x_i-1, and fracpartial r_ipartial p may then be defined analytically as","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"$","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"\\frac{\\partial ri}{\\partial x{i+1}} = I - (t{i+1} - t{i}) \\frac{\\partial f}{\\partial x{i}} \\left( \\frac{xi + x{i+1}}{2}, p, ti + \\frac{t{i+1} - t{i}}{2} \\right) \\left( \\frac{1}{2} \\right) $ ","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"$","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"\\frac{\\partial ri}{\\partial x{i}} = -I - (t{i+1} - t{i}) \\frac{\\partial f}{\\partial x{i}} \\left( \\frac{xi + x{i+1}}{2}, p, ti + \\frac{t{i+1} - t{i}}{2} \\right)\\left( \\frac{1}{2} \\right) $ ","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"$","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"\\frac{\\partial ri}{\\partial p} = - (t{i+1} - t{i}) \\frac{\\partial f}{\\partial p} \\left( \\frac{xi + x{i+1}}{2}, p, ti + \\frac{t{i+1} - t{i}}{2} \\right) $","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The jacobians fracpartial fpartial x and fracpartial fpartial p may be provided analytically or computed using numerical or automatic differentiation.","category":"page"},{"location":"theory/#DABDF2-with-an-Implicit-ODE","page":"Theory","title":"DABDF2 with an Implicit ODE","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"The residual function for each time step takes the following form $ r(x^{i-2 \\cdots i}, p, t^{i-2 \\cdots i}) = f\\left( \\gamma \\left( ti - t{i-1} \\right) \\left(x{i} + \\left(c - 1 \\right) x{i-1} - c x{i-2}\\right), x{i}, p, t{i}\\right) $  where  $ \\rho = \\frac{ti - t{i-1}}{t{i-1} - t_{i-2}} \\quad \\gamma = \\frac{1+\\rho}{1+2\\rho} \\quad c = \\frac{\\rho^2}{1 + 2 \\rho} $ and f is our underlying implicit ordinary differential equation.  Note that this residual function depends on the state variables from the i-2 iteration, so the adjoint equations must be modified accordingly.","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"The jacobians fracpartial r_ipartial x_i, fracpartial r_ipartial x_i-1, fracpartial r_ipartial x_i-2, and fracpartial r_ipartial p may be defined analytically as","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"$","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"\\frac{\\partial ri}{\\partial x{i}} = \\frac{\\partial f}{\\partial \\dot{x}} \\left( \\gamma \\left( ti - t{i-1} \\right) \\left(x{i} - \\left(c - 1 \\right) x{i-1} + c x{i-2}\\right), x{i}, p, t{i}\\right)\\gamma \\left( ti - t{i-1} \\right) + \\frac{\\partial f}{\\partial x} \\left( \\gamma \\left( ti - t{i-1} \\right) \\left(x{i} - \\left(c - 1 \\right) x{i-1} + c x{i-2}\\right), x{i}, p, t{i}\\right) $ ","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"$","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"\\frac{\\partial ri}{\\partial x{i-1}} = \\frac{\\partial f}{\\partial \\dot{x}} \\left( \\gamma \\left( ti - t{i-1} \\right) \\left(x{i} - \\left(c - 1 \\right) x{i-1} + c x{i-2}\\right), x{i}, p, t{i}\\right)\\gamma \\left( ti - t_{i-1} \\right)(c-1) $ ","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"$","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"\\frac{\\partial ri}{\\partial x{i-2}} = \\frac{\\partial f}{\\partial \\dot{x}} \\left( \\gamma \\left( ti - t{i-1} \\right) \\left(x{i} - \\left(c - 1 \\right) x{i-1} + c x{i-2}\\right), x{i}, p, t{i}\\right)\\gamma \\left( ti - t_{i-1} \\right)(-c) $ ","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"$","category":"page"},{"location":"theory/","page":"Theory","title":"Theory","text":"\\frac{\\partial ri}{\\partial p} = \\frac{\\partial f}{\\partial p} \\left( \\gamma \\left( ti - t{i-1} \\right) \\left(x{i} - \\left(c - 1 \\right) x{i-1} + c x{i-2}\\right), x{i}, p, t{i}\\right) $","category":"page"},{"location":"theory/#Comparison-with-Automatic-Differentiation","page":"Theory","title":"Comparison with Automatic Differentiation","text":"","category":"section"},{"location":"theory/","page":"Theory","title":"Theory","text":"The discrete adjoint may also be implemented by using reverse-mode automatic differentiation across the entire time integration process.  In practice, however, compiling time and memory requirements for reverse-mode automatic differentiation are often prohibitively high when applied across the entire time integration process.  A combination of analytic expressions and automatic differentiation may therefore offer the best solution.","category":"page"},{"location":"#DiscreteAdjoint","page":"Home","title":"DiscreteAdjoint","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"(Image: Stable) (Image: Dev) (Image: Build Status)","category":"page"},{"location":"","page":"Home","title":"Home","text":"A General Purpose Implementation of the Discrete Adjoint Method","category":"page"},{"location":"","page":"Home","title":"Home","text":"Author: Taylor McDonnell","category":"page"},{"location":"","page":"Home","title":"Home","text":"DiscreteAdjoint is a general purpose implemenation of the discrete adjoint method, which has been designed for use with OrdinaryDiffEq.  The approach taken by this package is to combine analytic expressions with automatic differentiation in order to construct a fast, but general implementation of the discrete adjoint method.  While SciMLSensitivity also provides methods for sensitivity analysis, we have found that the adjoint method implementation in this package is less computationally expensive than the methods provided by the SciMLSensitivity package, while still maintaining all the benefits of the discrete adjoint over the continuous adjoint. Specific details about the performance of this package relative to the various sensitivity analysis methods provided by the SciMLSensitivity package may be found in the benchmark folder.","category":"page"},{"location":"#Development-Status","page":"Home","title":"Development Status","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"This package is still a work in progress, and therefore only supports a small subset of the algorithms provided by OrdinaryDiffEq.  However, most algorithms provided by the OrdinaryDiffEq package can be supported by this package with a little bit of work.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Additionally, this package currently lacks support for some of the features included by the SciMLSensitivity package including callback tracking, checkpointing, and automatic differentiation integration (through Zygote), though these features may be added in future releases of this package.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Feel free to open a pull request if you wish to add an additional algorithm or otherwise contribute to this package's development.","category":"page"},{"location":"#Installation","page":"Home","title":"Installation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Enter the package manager by typing ] and then run the following:","category":"page"},{"location":"","page":"Home","title":"Home","text":"pkg> add https://github.com/byuflowlab/DiscreteAdjoint.jl","category":"page"},{"location":"#Supported-Algorithms","page":"Home","title":"Supported Algorithms","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Currently the following algorithms (from OrdinaryDiffEq) are supported:","category":"page"},{"location":"","page":"Home","title":"Home","text":"For Non-Stiff Ordinary Differential Equations:","category":"page"},{"location":"","page":"Home","title":"Home","text":"BS3\nOwrenZen3\nDP5\nTsit5","category":"page"},{"location":"","page":"Home","title":"Home","text":"For Fully-Implicit Differential Algebraic Equations","category":"page"},{"location":"","page":"Home","title":"Home","text":"DImplicitEuler","category":"page"},{"location":"","page":"Home","title":"Home","text":"Note that DAE initialization algorithms are not yet supported, though this only impacts  the gradient of the objective with respect to the initial conditions.","category":"page"},{"location":"#Example-Usage","page":"Home","title":"Example Usage","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"See the example usage","category":"page"}]
}
